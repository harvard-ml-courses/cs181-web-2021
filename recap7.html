---
layout: page
title: Lecture 7 Recap - Model Selection (Bayesian)
mathjax: true
weight: 0
---

<section class="main-container text">
    <div class="main">
      <h4>Date: February 16, 2021 (<a href="https://docs.google.com/forms/d/e/1FAIpQLSfOFrH5hFSp5mnV53qS4lTWFzm5pDAY_BLhMlIs-pl-GHAw-Q/viewform" target="_blank">Concept Check</a>, <a href="https://docs.google.com/forms/d/e/1FAIpQLSfOFrH5hFSp5mnV53qS4lTWFzm5pDAY_BLhMlIs-pl-GHAw-Q/viewanalytics" target="_blank">Class Responses</a>, <a href="{{ site.baseurl }}/ccsolutions" target="_blank">Solutions</a>)</h4>
      <h4>Relevant Textbook Sections: 2.8, 2.9</h4>

      <br>

      <h3>Lecture 7 Summary</h3>

      <ul>
        <li><a href="#recap7_1">Diversion: More Real World Context on Model Selection</a></li>
        <li><a href="#recap7_2">Intro to Bayesian Model Selection</a></li>
        <!-- <li><a href="#recap7_3">Concept 1 - What we now have: The Posterior Distribution</a></li>
        <li><a href="#recap7_4">Concept 2 - What we want to find: The Posterior Predictive Distribution</a></li>
        <li><a href="#recap7_5">Concept 3 - How to choose the best model: The Marginal Likelihood</a></li> -->
        <li><a href="#recap7_6">Example using the Beta-Bernoulli Model</a></li>
        <li><a href="#recap7_7">Bayesian Occam's Razor</a></li>
      </ul>

      <h2 id="recap7_1">Diversion: More Real World Context on Model Selection</h2>

      We start the lecture with a few more real world examples to discuss. If inaccurate assumptions are made, models can be wrong even when all of the math is correct. For instance, we might assume that data features are independent of each other when the real-world features are actually correlated. 
      
      <br><br>

      Another example is a medical model assuming that antidepressants and patient features produce a recovery outcome, and that a correct drug could be selected for each patient to help them recover. Machine learning was used to predict antidepressant recommendations, but the model ended up overfitting. In the data, some patients’ illnesses were just less severe. These patients were more likely to get better no matter what medication they were prescribed. The model would predict medications and these patients would probably get well, but not because it was the best drug — this was just an easy patient to treat.

      <br><br>

      <h2 id="recap7_2">Intro to Bayesian Model Selection</h2>

      When we talked about classification, we had discriminative and generative models. Each one makes sense in different scenarios.

      <ul>
      <li>Discriminative: Use $x$ to predict $y$ by maximizing $p(y | x)$. As an example, we might use data about a customer’s features ($x$) to predict what they will buy ($y$).</li>
      
      <li>Generative: Consider how the data is being produced by modeling $p(x,y)$. As an example, we might have data about patient symptoms ($y$) and want to know what disease produced them ($x$).</li>
      </ul>
      Earlier in Lecture 3, we discussed a generative setting where data was produced via the following story:
      <!-- TODO: ADD LECTURE 3 HREF -->
      
      <ol>
        <li>We have an input $x$</li>
        <li>We have a noise sample $\epsilon \sim \mathcal{N}(0,\sigma^2)$</li>
        <li>The data $y$ is produced as $y = w^Tx + \epsilon$</li>
      </ol>
      
      In the Lecture 3 story, $w$ was fixed. Today, we will add a step 0 where we consider $w$ to be a random variable as well. (The $w$’s are the different models we are considering. For example, they might represent all possible decision boundary lines. Through the Bayesian view, we can look at three useful concepts: the posterior distribution, the posterior predictive, and the marginal likelihood.
      
      <ul>
        <li>
          <B>Posterior distribution.</B> After obtaining some data $X, y$, we can compute the posterior distribution $p(w | X,y)$. This tells us, “Given all the data you’ve seen in $X$ and $y$, these are the possibilities (and probabilities) for what $w$ will look like.”

          <br><br>
          
          How can we find this distribution? First observe that in the data generation story we’re currently considering, we only need to know $w$, $x’$, and $\epsilon$ to get $y’$. So, we can make simplifications such as $p(y’ | w, x’, X, y) = p(y’ | w, x’)$, because the value of $y’$ does not depend on previously observed data. Also, seeing more $x$’s alone does not tell us more about what $w$ is, but seeing more labeled pairs of $x$’s and $y$’s together does tell us more about $w$’s distribution. Thus, $p(w|X) = p(w)$, but $p(w|X, y) \neq p(w)$.

          <br><br>
          
          Using this along with the fact that $p(y | X)$ does not depend on $w$, we can apply Bayes’ Theorem and then do some simplifying:

          \begin{align*}
            p(w | X,y) &= \frac{p(y | X, w)p(w | X)}{p(y | X)}
            \\ &= \frac{p(y | X, w)p(w)}{p(y | X)}
            \\ &\propto p(y | X,w)p(w)
          \end{align*}

          Depending on the setting and prior, this could be an easy calculation (if there's conjugacy) or a hard calculation (if there's not).

          <br><br>

          <u>Note:</u> Conjugacy is when the prior and posterior are from the same family of distributions (e.g. both Gaussian, or both Beta). It can provide nice mathematical properties and cleaner forms that make optimization later on much easier. 
        </li>

        <li>

          <B>Posterior predictive.</B> Improving our understanding of $w$’s distribution is nice, but to actually make predictions, we use the posterior predictive $p(y’ | x’, X, y)$. For a new piece of test data $x’$, the posterior predictive $p(y’ | x’, X, y)$ tells us how likely the label for $x’$ is to be $y’$ given the data $X, y$ that we have already observed. To find the posterior predictive, we would integrate over the possible $w$’s in our posterior distribution for $w$:

          $$p(y’ | x’, X, y) = \int_w p(y’ | w, x’)p(w | X, y)dw$$ 

          <u>Note:</u> In the Bayesian framework, we avoid working directly with $w$. It’s a random variable we don’t know the value of, so instead of working with $w$, we integrate over the whole distribution for $w$ to consider the likelihoods of all the different possible values.

        </li>

        <li>
          <B>Marginal likelihood.</B> Finally, we can compute the marginal likelihood, the probability of the training set

          $$p(x,y) = \int_w p(y | X,w)p(w)dw$$

          The posterior predictive is what we use for inference (making predictions), but maximizing the marginal likelihood is what helps us select a model. 
          <!-- get what a 'model' is clarified -->
        </li>
      </ul>

      
      <!-- <h2 id="recap7_3">Concept 1 - What we now have: The Posterior Distribution</h2>

      First, the Bayesian view allows us to look at the posterior distribution. The posterior distribution essentially is a summary of what we know after we've seen the data (as in, given data X, Y):

      $$p(w|X, Y)$$

      The above is the posterior over models. It essentially tells us, given all the data we've seen in X and Y, these are the possibilities of what w will look like (described in a distribution format).<br><br>

      <u>Note: How would we calculate this?</u><br><br>

      First we use Bayes' rule:

      $$p(w|X, Y) = p(Y|X, w) p(w|X) / p(Y | X)$$

      We notice that p(w|X) is equivalent to p(w) as simply seeing X won't change our info on w. So essentially, we can start with just the prior on w.

      $$p(w|X, Y) = p(Y|X, w) p(w) / p(Y | X)$$

      The denominator is a constant with respect to w.

      $$p(w|X, Y) \propto p(Y|X, w) p(w)$$

      Now, depending on the likelihood and prior, this could be an easy calculation (if there's conjugacy) or a hard calculation (if there's not). -->
<!-- 
      <h2 id="recap7_4">Concept 2 - What we want to find: The Posterior Predictive Distribution</h2>

      While it is nice to have the posterior distribution, what we ultimately are looking for is the ability to make a prediction for the target $y^{*}$ given a new input $x^{*}$. To do this, we write the expression below and integrate over w. Intuitively, this makes sense as we are just looking at each individual w (which represents different models), using that w to make a prediction for y, and then weighting each model by the probability of it happening.<br><br>

      $$p(y^{*} | x^{*}, X, Y) = \int p(y^{*} | x^{*}, w)p(w|X, Y)dw$$

      <u>Student Question: Why is there no x* in the p(w|X,Y) expression?</u> It's because our model hasn't been trained on the new input, so the distribution we have on the weights only depends on the data we've seen, the X and Y.

      <h2 id="recap7_5">Concept 3 - How to choose the best model: The Marginal Likelihood</h2>

      To choose the best model, we ultimately want to look at the marginal likelihood. In other words, we want to find the probability of Y given X.

      $$p(Y | X) = \int p(Y|X, w) p(w) dw$$

      We can break it down into the above, by integrating over all the possible w's. Overall, the model with the highest p(Y | X) is the model you want. It's essentially saying, given the prior on w that we have, let's look at each w and find the likelihood of Y for each w and weight it by the corresponding probablity of w, leading us to get a marginal (also known as integrated) likelihood of Y overall. The model with the highest likelihood, is a way of selecting the best model. -->

      <h2 id="recap7_6">Example using the Beta-Bernoulli Model</h2>

      <h3>1. Defining the Bernoulli Part</h3>

      Let's consider a coin that comes up "1" with probability $\theta$. Let $x$ be the result of the toss. This is the Bernoulli PMF:

      $$p(x | \theta) = \theta^x (1 - \theta)^{1 - x}$$

      If we want to write out the probability of multiple coin flips, then we have:

      $$p(x_1 ... x_n | \theta) = \prod_n (\theta)^{x_n}(1-\theta)^{1 - x_n}$$

      $$p(x_1 ... x_n | \theta) = \theta^{n_1}(1-\theta)^{n_0}$$

      Where in the above, $n_1$ is the number of heads, and $n_0$ is the number of tails.<br><br>

      <u>Side Note:</u> at this point, we could've taken the maximum likelihood frequentist approach as follows $\theta_{MLE} = argmax_{\theta}$ $\theta^{n_1}(1-\theta)^{n_0}$, which leads us to an intuitive answer: $n_1/(n_1 + n_0)$<br><br>

      <h3>2. Defining the Beta Part</h3>

      In the Bayesian approach, we have $p(\theta)$ ~ $Beta(\alpha, \beta)$<br><br>

      This is the PDF: $p(\theta) = \frac{1}{B(\alpha, \beta)} \theta^{\alpha-1}(1 - \theta)^{\beta - 1}$. Conveniently this is conjugate to our likelihood (you can tell by eyeballing it too that looks a lot like our likelihood), leading to an easy calculation in the next step (this is the same thing as mentioned above, where when there is conjugacy, then there is an easier calculation).

      <h3>3. Calculating the Posterior</h3>

      As from before, we have:

      $$p(\theta | X) \propto p(X|\theta)p(\theta)$$

      $$\theta^{\alpha - 1 + n_1} (1 - \theta)^{\beta - 1 + n_0}$$

      We've multiplied the likelihood with the prior (here, the PDF of a Beta). Since we are looking at proportionality, we can drop the constants.<br><br>

      We then add in a normalization factor of $\frac{1}{B(\alpha + n_1, \beta + n_0)}$, which we can do by matching on to the same pattern from before. We do this because this is a probability function, so we need to make sure the sum of the probabilities comes out to 1.<br><br>

      <h3>4. A Pause for Intuition</h3>

      Let's pause for our intuition on the prior of Beta. Let's say we had $\alpha = 1$ and $\beta = 1$. That would be equivalent to a prior of seeing 1 head and 1 tail, which (feel free to look up the Beta distribution on Google at this to help visualize the intuition), basically puts more weight on the $\theta$ being 0.5, which intuitively makes sense. If we had $\alpha = 3$ and $\beta = 3$, then it'd be an even more extreme version of the same thing, with even more weight on 0.5, because we're even more sure it's 0.5. But if it were like $\alpha = 3$ and $\beta = 1$, then the $\theta$ distribution would have most of it's weight towards the right, with a higher chance that $\theta = 1$, which again, makes intuitive sense as we've seen 3 heads and only one tail.<br><br>

      <h3>5. Calculating the MAP</h3>

      We can at this point, choose maximize the posterior, something we've talked over in the course previously.

      $$\theta_{MAP} = argmax_{\theta} p(\theta | x) = \alpha - 1 + n_1 / (\alpha + \beta + n_0 + n_1 - 2)$$

      For the math, we omit it because we can basically just pattern match to the max likelihood calculation before, it looks exactly the same in format. The intuition here is that as n gets large, the data will overwhelm whatever prior you have. This almost feels like regularization really, where we're trying to pull back and not look purely at the data, but give a gentle pull closer to 0.5 (if we set our $\alpha$ and $\beta$ that way), almost like a gentle tug. It's important to note that this isn't taking advantage of all the information in the posterior, but rather simply a maximum.<br><br>

      <h3>6. Calculating the Posterior Predictive</h3>

      The most Bayesian person would actually go look for the posterior predictive distribution.

      $$p(x = 1|X) = \int p(x=1|\theta) p(\theta | x_1 ... x_n )$$

      $$p(x = 1|X) = \int \theta \textrm{ } p(\theta | x_1 .. x_n)$$

      Now it so happens to be that we're taking the expected value of theta because we used the trick to start the probabiliyt of heads with $\theta$, so we can use that information to calculate this term. It's very important to realize that we chose this purely out of covenience. In a real life situation, the probabilty of heads very likely won't be set perfectly in that way. Now we can calculate as follows:

      $$E[\theta] = (\alpha + n_1)/(\alpha + \beta + n_1 + n_0)$$

      <h3>7. Summary and Final Example</h3>

      Example: Suppose $\alpha = \beta = 1$ as our prior. Then, we see 2 heads.

      $$p(x = 1 | \theta_{MLE}) = 1$$

      $$p(x = 1 | \theta_{MAP}) = 1$$

      Note: for intuition on the term above, if you look at beta distribution, $\alpha$' is now 3, and $\beta$' is 1, it looks like an increasing curve from left to right, with the peak at 1, so the final value still comes out at 1.

      $$p(x = 1| \textrm{2 heads}, \alpha, \beta) = 3/4$$

      <u>Student Question: Why is the MLE 1?</u> We had 2 head flips, we don't consider any prior, and so we divide 2 head flips over 2 total flips and get 1.

      <h2 id="recap7_7">Bayesian Occam's Razor</h2>

      The idea of Occam's Razor is often times, the simpler model is better (amongst the feasible choices). The intuition behind this in the Bayesian sense is as follows. <br><br>

      First, we remember that in the Bayesian view, a common theme we've kept seeing is that we're essentially averaging over all possible models when we make our predictions. This is important because this influences how we select our model class. If we select a more complicated model class, say quadratic, because the size of the model class is big, the probability of any particular model being the right model is smaller, as the total sum of probabily is 1 no matter what, so each model is spread thin and has a small probabilty of actually being the best model. As we move from quadratric to linear, the probability of any given model of a linear class is naturally higher, since there are less possible models (model class is smaller) in the first place. Finally, if we go to a constant based model, the model class is even smaller, leading to an even higher probability per model.<br><br>

      Essentially, we have the following idea. Obviously, if our data is in any way complicated, we can't pick the constant based model, because that is way too simple to model our data. However, amongst the feasible choices (say between linear and quadratic), we should always pick the simpler model class. In the simpler model class, there are less possibilities in an overall smaller model class size, so each individual model is more likely to occur.
    </div>
</section>

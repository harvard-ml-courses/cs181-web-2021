---
layout: page
title: Lecture 5 Recap - Probabilistic Classification
mathjax: true
weight: 0
---

<section class="main-container text">
    <div class="main">
      <h4>Date: February 9, 2021 (<a href="https://forms.gle/sUtMqHxjfN4r718A8" target="_blank">Concept Check</a>, <a href="https://docs.google.com/forms/d/e/1FAIpQLSd5v302N_cg8cp60oX2PvX3yvV3CjpLiQ1G-NGNkusoJDSRag/viewanalytics" target="_blank">Class Responses</a>, <a href="{{ site.baseurl }}/ccsolutions" target="_blank">Solutions</a>)</h4>
      <h4>Relevant Textbook Sections: 3.6</h4>
      <h4>Cube: Supervised, Discrete/Continuous, Probabilistic</h4>

      <br>

      <h4><a href="https://harvard.zoom.us/rec/play/Q88I40zXjvku3_HdYVvF3WqrqzT40SCAy2tbOB8Z705ZDT5LqNpPu_EUXp_NmglGNfP29B9asrjNI7YE.B3vPn9CY2QKc6Oq3">Lecture Video</a></h4>
      <h4><a href="files/lecture5_slides.pdf" target="_blank">Slides</a></h4>
      <h4><a href="files/lecture5_ipad.pdf" target="_blank">iPad Notes</a></h4>

      <h3>Lecture 5 Summary</h3>

      <ul>
        <li><a href="#recap5_1">Motivation</a></li>
        <li><a href="#recap5_2">Probabilistic Classification Overview</a></li>
        <li><a href="#recap5_3">Discriminative Approach</a></li>
        <li><a href="#recap5_4">Generative Approach</a></li>
      </ul>

      <br>

      <h2 id="recap5_1">Motivation</h2>

      To motivate our exploration of probabilistic regression, let's explore some example classification problems:<br>

      <ul>
            <li>Classifying an email as spam.</li>
            <li>Predicting parking bans in inclement weather.  What is the probability there will be greater than 5 inches of snow?</li>
            <li>Deciding whether or not to show a user an ad.</li>
            <li>Deciding whether an individual should quarantine. What is the probability an individual has COVID-19?</li>
            <li>Dating platform algorithms. What is the probability that person 1 will like person 2?</li>
      </ul>

      As illustrated in several of these examples, we often are not only interested in predicting class labels, but are also interested in approximating the <i>probability</i> that an example belongs to a particular class.<br><br>

      Recall that in <a href="{{ site.baseurl }}/recap4">last week's lecture</a>, we used the below parametric classification model to predict label $\hat{y}_n$ as <br>

      \begin{equation}
        \hat{y}_n=\left\{
        \begin{array}{@{}ll@{}}
          +1, & \text{if}\ \mathbf{w}^T \mathbf{x} + w_0 > 0 \\
          -1, & \text{otherwise}
        \end{array}\right.
      \end{equation}

      We learn weights $\mathbf{w}$ by training using the hinge loss.<br><br>

      But, what if we're interested in calculating a <i>probability</i> that an example is positive?  Today we'll discuss two approaches.  In both cases, we're going to use <i>maximum likelihood estimation</i> to learn model parameters.<br><br>

      <h2 id="recap5_2">Probabilistic Classification Overview</h2>

      Today we'll discuss two different approaches to probabilistic classification: the discriminative and the generative approach.<br><br>

      <h3>Approach 1:  Discriminative</h3>

      Our goal is to find parameters $\mathbf{w}$ that maximize the <i>conditional probability</i> of labels in the data: <br>

      $$\argmax_{\mathbf{w}} \prod_n p(y_n | \mathbf{x}_n, \mathbf{w})$$

      The term $p(y_n | \mathbf{x}_n, \mathbf{w})$ is called the conditional likelihood.<br><br>

      In this setting, the labels $y_n$ are generated based on covariates or features $\mathbf{x}_n$.  We take the product in this expression because we think of individual pairs $\{(\mathbf{x}_n, y_n)\}_{n = 1}^N$ as independently and identically distributed.<br><br>

      We'll illustrate the discriminative approach using a method called <i>logistic regression</i>.<br><br>

      <h3>Approach 2:  Generative</h3>

      Our goal is to find parameters $\mathbf{w}$ that maximize the <i>joint distribution</i> of both the features $\mathbf{x}_n$ and labels $y_n$.

      $$\argmax_{\mathbf{w}} \prod_n p(\mathbf{x}_n, y_n | \mathbf{w})$$

      The term $p(\mathbf{x}_n, y_n | w)$ is called the joint likelihood.<br><br>

      In general, while the discriminative approach is simple, the generative approach is flexible.  The generative approach can add knowledge and handle missing labels quite elegantly.<br><br>

      We'll illustrate the generative approach using two methods: <i>multi-variate Gaussian</i> models and <i>Naive Bayes</i> models.<br><br>

      A few notes:
      <ul>
            <li>Today for convenience we will assume the two classes are $\{0, 1\}$, where $1$ is the "positive" and $0$ is the "negative" label.</li>
            <li>A new example $y$ is 1 if the probability that it is 1 is greater than the probability that it is 0.

            \begin{equation}
              \hat{y} =\left\{
              \begin{array}{@{}ll@{}}
                1, & \text{if}\ p(y = 1 | \mathbf{x}) > p(y = 0 | \mathbf{x}) \\
                0, & \text{otherwise}
              \end{array}\right.
            \end{equation}

            </li>
      </ul>


      <h2 id="recap5_3">Discriminative Approach</h2>

      <h2 id="recap5_4">Generative Approach</h2>


    </div>
</section>

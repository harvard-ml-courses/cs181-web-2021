---
layout: page
title: Lecture 5 Recap - Probabilistic Classification
mathjax: true
weight: 0
---

<section class="main-container text">
    <div class="main">
      <h4>Date: February 9, 2021 (<a href="https://forms.gle/sUtMqHxjfN4r718A8" target="_blank">Concept Check</a>, <a href="https://docs.google.com/forms/d/e/1FAIpQLSd5v302N_cg8cp60oX2PvX3yvV3CjpLiQ1G-NGNkusoJDSRag/viewanalytics" target="_blank">Class Responses</a>, <a href="{{ site.baseurl }}/ccsolutions" target="_blank">Solutions</a>)</h4>
      <h4>Relevant Textbook Sections: 3.6</h4>
      <h4>Cube: Supervised, Discrete/Continuous, Probabilistic</h4>

      <br>

      <h4><a href="https://harvard.zoom.us/rec/play/Q88I40zXjvku3_HdYVvF3WqrqzT40SCAy2tbOB8Z705ZDT5LqNpPu_EUXp_NmglGNfP29B9asrjNI7YE.B3vPn9CY2QKc6Oq3">Lecture Video</a></h4>
      <h4><a href="files/lecture5_slides.pdf" target="_blank">Slides</a></h4>
      <h4><a href="files/lecture5_ipad.pdf" target="_blank">iPad Notes</a></h4>

      <h3>Lecture 5 Summary</h3>

      <ul>
        <li><a href="#recap5_1">Motivation</a></li>
        <li><a href="#recap5_2">Probabilistic Classification Overview</a></li>
        <li><a href="#recap5_3">Discriminative Approach</a></li>
        <li><a href="#recap5_4">Generative Approach</a></li>
      </ul>

      <br>

      <h2 id="recap5_1">Motivation</h2>

      To motivate our exploration of probabilistic regression, let's explore some example classification problems:<br>

      <ul>
            <li>Classifying an email as spam.</li>
            <li>Predicting parking bans in inclement weather.  What is the probability there will be greater than 5 inches of snow?</li>
            <li>Deciding whether or not to show a user an ad.</li>
            <li>Deciding whether an individual should quarantine. What is the probability an individual has COVID-19?</li>
            <li>Dating platform algorithms. What is the probability that person 1 will like person 2?</li>
      </ul>

      As illustrated in several of these examples, we often are not only interested in predicting class labels, but are also interested in approximating the <i>probability</i> that an example belongs to a particular class.<br><br>

      Recall that in <a href="{{ site.baseurl }}/recap4">last week's lecture</a>, we used the below parametric classification model to predict label $\hat{y}_n$ as <br>

      \begin{equation}
        \hat{y}_n=\left\{
        \begin{array}{@{}ll@{}}
          +1, & \text{if}\ \mathbf{w}^T \mathbf{x} + w_0 > 0 \\
          -1, & \text{otherwise}
        \end{array}\right.
      \end{equation}

      We learn weights $\mathbf{w}$ by training using the hinge loss.<br><br>

      But, what if we're interested in calculating a <i>probability</i> that an example is positive?  Today we'll discuss two approaches.  In both cases, we're going to use <i>maximum likelihood estimation</i> to learn model parameters.<br><br>

      <h2 id="recap5_2">Probabilistic Classification Overview</h2>

      Today we'll discuss two different approaches to probabilistic classification: the discriminative and the generative approach.<br><br>

      <h3>Approach 1:  Discriminative</h3>

      Our goal is to find parameters $\mathbf{w}$ that maximize the <i>conditional probability</i> of labels in the data: <br>

      $$\operatorname*{argmax}_{\mathbf{w}} \prod_n p(y_n | \mathbf{x}_n, \mathbf{w})$$

      The term $p(y_n | \mathbf{x}_n, \mathbf{w})$ is called the conditional likelihood.<br><br>

      In this setting, the labels $y_n$ are generated based on covariates or features $\mathbf{x}_n$.  We take the product in this expression because we think of individual pairs $\{(\mathbf{x}_n, y_n)\}_{n = 1}^N$ as independently and identically distributed.<br><br>

      We'll illustrate the discriminative approach using a method called <i>logistic regression</i>.<br><br>

      <h3>Approach 2:  Generative</h3>

      Our goal is to find parameters $\mathbf{w}$ that maximize the <i>joint distribution</i> of both the features $\mathbf{x}_n$ and labels $y_n$.

      $$\operatorname*{argmax}_{\mathbf{w}} \prod_n p(\mathbf{x}_n, y_n | \mathbf{w})$$

      The term $p(\mathbf{x}_n, y_n | w)$ is called the joint likelihood.<br><br>

      In general, while the discriminative approach is simple, the generative approach is flexible.  The generative approach can add knowledge and handle missing labels quite elegantly.<br><br>

      We'll illustrate the generative approach using two methods: <i>multi-variate Gaussian</i> models and <i>Naive Bayes</i> models.<br><br>

      A few notes:
      <ul>
            <li>Today we will assume the two classes are $\{0, 1\}$, where $1$ is the "positive" and $0$ is the "negative" label.</li>
            <li>A new example $y$ has predicted label 1 if the probability that it is 1 is greater than the probability that it is 0.

            \begin{equation}
              \hat{y} =\left\{
              \begin{array}{@{}ll@{}}
                1, & \text{if}\ p(y = 1 | \mathbf{x}) > p(y = 0 | \mathbf{x}) \\
                0, & \text{otherwise}
              \end{array}\right.
            \end{equation}

            </li>
      </ul>

      <h2 id="recap5_3">Discriminative Approach</h2>

      In logistic regression, we model $p(y | \mathbf{x})$ using the sigmoid (or "logistic") function:

      $$p(y = 1 | \mathbf{x}) = \frac{1}{1 + \exp(- \mathbf{w}^T \mathbf{x})}$$

      The sigmoid (or logistic) function is often denoted using a $\sigma$, and takes scalar values as input.  The sigmoid function intuitively "flattens" its input to output values between $0$ and $1$:<br><br>

      <div class="text-center">
        <img src="{{ site.baseurl }}/images/recap5_1.png" style="width:40%"  alt="chart of sigmoid(z)"></img>
      </div>

      In logistic regression, $p(y = 0 | \mathbf{x}) := 1 - p(y = 1 | \mathbf{x})$.<br><br>

      If we write $h = \mathbf{w}^T \mathbf{x}$, then

      $$ p(y = 1 | \mathbf{x}) = \frac{1}{1 + e^{-h}}, p(y = 0 | \mathbf{x}) = \frac{1}{1 + e^h}$$

      Since $y \in \{0, 1\}$, we can rewrite $p(y | \mathbf{x})$ using the "power trick":

      $$ p(y | \mathbf{x}) = p( y = 1 | \mathbf{x})^y \cdot p(y = 0 | \mathbf{x})^{1 - y}$$

      The parameters <i>maximize</i> a likelihood function (the maximum likelihood estimate) also <i>minimize</i> the negative log-likelihood.  Thus, we can treat the negative log-likelihood - the expression we are minimizing - as a "loss function".

      Therefore the negative log-likelihood, or "loss", over the dataset $D$ $L_D(\mathbf{w})$ can be written as:

      $$ L_D(\mathbf{w}) = - \sum_n \ln [p(y_n | \mathbf{x}_n)]$$

      Applying log rules and the power trick,

      $$ = - \sum_n y_n \ln [\sigma(-h_n)] - \sum_n (1 - y_n) \ln [\sigma(h_n)]$$
      $$ = \sum_n y_n \ln [1 + \exp(-h_n)] + \sum_n (1 - y_n) \ln [1 + \exp(h_n)]$$

      The second term $(1 - y_n) \ln [1 + \exp(h_n)]$ can be understood as the loss of the classifier on a negative example $(\mathbf{x}_n, y_n = 0)$.  We can draw a picture of this logistic loss function $(1 + \exp(\mathbf{w}^T x))$.  It is helpful to compare this plot of logistic loss to that of 0/1 loss and hinge loss, which we discussed last week.<br><br>

      <div class="text-center">
        <img src="{{ site.baseurl }}/images/recap5_2.png" style="width:40%"  alt="chart displaying 0-1, hinge, and logistic loss"></img>
      </div>

      The logistic loss function is differentiable and convex.  This means we can use gradient descent methods to minimize this loss.  Unlike the hinge loss function, it still penalizes predictions even when .  In a sense, it prefers to make better decisions on correct predictions, pushing away from the decision boundary.

      <h3>Stochastic gradient descent</h3>

      For Hinge loss, positive example:

      If $\hat{y_n} \neq y_n$, then $w_{y + 1} \leftarrow w_t + \eta x_n$.

      Negative example:  If $\hat{y_n} \neq y_n$, $w_{t + 1} \leftarrow w_{t} - \eta x_n$.

      Versus for logistic regression, even if $\hat{y}_n = y_n$, then

      Positive example: $$ w_{t + 1} \leftarrow w_t + \eta x_n p(y_n = 0 | x_n)$$

      Negative example: $$ w_{t + 1} \leftarrow w_t - \eta x_n p(y_n = 1 | x_n)$$

      Algorithms come from loss functions.

      Student question:  Why use gradient descent?  If it's differentiable and convex, can't we find an analtical solution?

      Answer:  In the case of logistic regression, it is hard to find an analytical solution.  Therefore in this course, we will use gradient descent to learn our logistic regression models.


      <h2 id="recap5_4">Generative Approach</h2>


    </div>
</section>

---
layout: page
title: Lecture 22 Recap - Reinforcement Learning 2
mathjax: true
weight: 0
---

<section class="main-container text">
    <div class="main">

      <h4>Date: April 20, 2021</h4>
      <br>

      <h4><a href="https://harvard.zoom.us/rec/play/JhHsjOm-YU_3KnhOd5INJnYG3UFZzu9XFMwDgEDa69NgF6Vd-vy4ia2S8Xcp85PDhrDByvQg53GO_8Gh.iHSPSrNlVx10LI6q">Lecture Video</a></h4>
      <h4><a href="files/lecture22_ipad.pdf" target="_blank">iPad Notes</a></h4>
      <h4><a href="files/lecture22_slides.pptx" target="_blank">Slides</a></h4>

      <h3>Lecture 22 Summary</h3>

      <ul>
        <li><a href="#recap22_1">Model-Based Learning</a></li>
        <li><a href="#recap22_2">Back to Model-Free Approaches</a></li>
        <li><a href="#recap22_3">Deep Q-Networks</a></li>
        <li><a href="#recap22_4">Policy Learning</a></li>
        <li><a href="#recap22_5">AlphaGo</a></li>
      </ul>

      <h2 id="recap22_1">Model-Based Learning</h2>

      <div class="text-center">
        <img src="{{ site.baseurl }}/images/recap22_1.png" style="width:70%"  alt="Model-Based Learning Diagram"></img>
      </div>

      In model-based learning, we cycle through taking actions, using data from those actions to learn a model, using the model to make plan, and then using the plan to continue acting. How do we handle exploring vs. exploiting in this framework? (Recall: in model-free RL last week, we used $\epsilon$-greedy action selection to balance exploration and exploitation.)<br><br>

      <b>Optimism under uncertainty.</b> Let $N(s,a)$ be the number of times we visited the state-action pair $(s,a)$. Each “visit” is a time that the agent was in state s and chose to perform action a. When we are learning, we will assume that if $N(s,a)$ is small, then the next state will have high reward. When we do this, we are being optimistic about the state-action pairs we’re uncertain about. Then, we plan using this optimistic model. Since the model thinks taking visiting lesser-known state-action pairs will lead to high reward, we’ll have a tendency to explore these lesser-known areas. <br><br>

      <b>Posterior sampling (Thompson sampling).</b> We maintain a posterior (Dirichlet) on the transition function $p(\cdot | s,a)$. We sample from this posterior to get a model, which we proceed to plan with. The idea is that when we are less certain about the outcome of the transition function, there’s a greater likelihood of the sampling leading us to explore. When we are more certain about what the best outcome is, we’re more likely to just stick with that best outcome.<br><br>

      <!-- In reinforcement learning, we've seen how it's hard to transfer things into the real world, especially with regards to how we define our reward functions. However, one application of RL that has developed a lot is in the context of playing games. This is because the rules and objectives are clear.<br><br>

      In the past, we thought that if we could make an AI play games really well, then it means we've created something that is smarter than humans and that was quite exciting at the time. Nowawadays, we've categorized AI's that can play games well as part of "Narrow AI", which is defined as AI that's programmed to do a single task (such as play a game, check the weather, etc), to be distinguished from "General AI", which refers to machines that exhibit actual human intelligence.<br><br>

      Despite this, it is still important to think through the evolution of Narrow AI, as this will highlight some of the technical challenges that were overcome for us to be able to play games well.<br><br>

      One application of RL in games was in Backgammaon (Gerald Tesauro). This was the first time an AI got to a world class expert level of performance, and the AI was an artificial neural net trained by a form of temporal difference (TD) learning, something we covered in last class. Because it used TD, the AI was named TD-Gammon. A more recent breakthrough took place in 2013, where DeepMind was able to master several atari games (a set of games including video pinball, breakout, robotank, crazy climber, and more). Next, we had AlphaGo. Something that's interesting about AlphaGo is just how differently Go was solved compared to a gamel like chess (DeepBlue). Whereas DeepBlue used a traditional method of doing a very deep search into the future (by considering if I do this, they'll do that, then if I do this, etc), in Go, it's simply impossible to search far enough. We can't even come close to bottoming out the game, and so instead, AlphaGo did some searching to some depth but then from that point onwards, learned if this "type" of game position was good to do this sort of move, or this other sort of move. It essentially developed a heuristic of good board positions vs bad positions, and over many many games played with itself, it eventually defeated the human Go world champion. -->

      <h2 id="recap22_2">Back to Model-Free Approaches</h2>

      <div class="text-center">
        <img src="{{ site.baseurl }}/images/recap22_2.png" style="width:70%"  alt="Model-Free Learning Diagram"></img>
      </div>

      In model-free RL, the cycle is different from model-based RL in that we are no longer learning a model and planning. Instead, feedback from the world is directly used to update the agent. Different Model-Free RL algorithms vary in how the agent selects its behavior (e.g. greedy or $\epsilon$-greedy) and how it learns (e.g. how it updates its Q-values).<br><br>

      Recall from last time our discussion on Q-values, SARSA, and Q-learning. We had a definition of the optimal Q-function, which could be used to find the optimal policy.

      $$Q^*(s,a) = r(s,a) + \gamma \sum_{s’}p(s’ | s, a)\max_{a’}Q^*(s’,a’)$$

      $$\pi^*(s) \in \arg\max_a Q^*(s,a)$$

      SARSA: On policy. Learns Q-values corresponding to the agent’s behavior, so what the agent learns depends on how the agent is behaving

      $$Q(s,a) \leftarrow Q(s,a) + \alpha_t(s,a)[r + \gamma Q(s’, a’) - Q(s,a)]$$

      Q_LEARN: Off policy. Learns Q-values $Q^*$ corresponding to the Bellman equation
      
      $$Q(s,a) \leftarrow Q(s,a) + \alpha_t(s,a)[r + \gamma \max_{a’}Q(s’, a’) - Q(s,a)]$$

      (For the next part of the lecture, we will require $\alpha_t(s,a) = 0$ unless $(s,a)$ was visited at $t$.)<br><br>

      When certain conditions are satisfied, we actually know for sure that Q_LEARN will converge to $Q^*$. We are not going to prove this theorem, but it comes from Bellman-equation, contraction-type ideas similar to some of what we discussed last week. 

      <h4>Q-Learning Convergence Theorem</h4>
      Q_LEARN converges to $Q^*$ as $t \rightarrow \infty$ as long as
      <ul>
        <li>$\sum_t \alpha_t(s,a) = \infty$ for all $s,a$. This means we need to visit each $(s,a)$ infinitely often. $\epsilon$-greedy helps with this by ensuring that all the state-action pairs still have a chance of being visited.</li>
        <li>$\sum_t \alpha^2(s,t) < \infty$ for all $s,a$. This can be achieved by reducing the the learning rate; e.g. $\alpha_t(s,a) = \frac{1}{N_t(s,a)}$.</li>
      </ul>

      For SARSA to converge to $Q^*$, we also need a third condition:
      <ul>
        <li>Behavior is “greedy in the limit”. That is, the behavior eventually follows the Q-values with no exploration. To satisfy this condition, we usually set $\epsilon_t(s_) = \frac{1}{N_t(s,a)}$, so $\epsilon$ is decreasing as we visit each state-action pair more often.</li>
      </ul>

      
      
      <!-- As a quick recap from last lecture, we mentioned there are three groups of approaches of doing reinforcement learning. We didn't finish our first group yet, our value-based methods, so we continue that today.<br><br>

      Specifically, last class we were looking at value-based methods for discrete state and action spaces. This lecture began with a long recap of last lecture, to make sure students were on the same page with regards to understanding. See time stamp 15:23 to 42:53 for the review. In the review, Finale also added one additional off-policy example to our value-based approaches in discrete space, which we cover below: -->

      <!-- <h4>Concluding Discrete Spaces with One More Off Policy Example</h4>

      Last class, we covered an on policy example (SARSA), and an off policy example (Q Learning) as part of our value-based approaches in discrete spaces. We intorduce one more example of an off policy example.<br><br>

      Imagine you're in a scenario where you know that there is a good policy out there. In the medical context, for example, there might be some well-researched policy that is good to follow for how to treat a patient. The key difference then, is our a', the same difference we saw between SARSA vs Q-learning. Where in SARSA, we used the a' from looking at our history and grabbing what action we actually took following our policy, and whereas in Q-learning, we used the argmax, as in the best $a^*$ of all the possible actions to exit $s'$, now we take a predefined policy when taking considering the which $a'$. As in, we have our external policy $\pi$, and we get our a' by doing $a' = \pi(s')$. Screenshot of math is below:

      
      This concludes our discrete state and action spaces. The rest of today, we will look at value-based methods for continuous spaces.

      <h4>Continuous Space</h4>

      So far, our value based methods for RL have been in discrete settings. Some might ask: why bother covering discrete settings like GridWorld? Doing GridWorld seemspointless and unapplicable in the real world. However, the reason it's important to cover gridworld is becaue while unapplicable, it builds very solid intuition for RL algorithms in general. The things that go wrong in simple settings still can go wrong in complex settings, so it's important for us to nail the simpler settings first.<br><br>

      The biggest difference between discrete and continuous settings is that now, our Q Table can't exist because it's continuous: we cant just hold a table of states by actions. How can we replicate the Q(s,a) from SARSA and Q-Learning? We need to approxmiate Q(s, a) with some function class: for example, a deep network. We also still need to be able to learn the Q(s, a) function given histories. Below are two approaches: -->

      <h2 id="recap22_3">Deep Q-Networks</h2>

      By combining Q-learning with neural networks, researchers have been able to achieve amazing results like learning how to <a href=“https://arxiv.org/abs/1312.5602”>play video games</a>. In an Atari game, each state  is comprised of 4 $84 \times 84$ pixel arrays representing the video game screen at times $t-3, t-2, t-1, t$. One frame can provide information about the current locations of objects in the game, but we need multiple frames to capture attributes like velocity and acceleration. After around 500 iterations of training on the game Breakout, where the player hits a ball upwards to break through a wall of blocks, the RL agent was able to find a clever game strategy that even the engineers working on the project didn’t know about!<br><br>

      Representing all of the game states in a table doesn’t work - there’s just too many states. So to parameterize $Q(s, a ; w)$, we use a differentiable deep network with parameters $w$. This network will take in a state and generate predictions for the Q-values of taking each action from this state. In this framework, we are trying to use our gradient descent updates to find a model that minimizes the squared error between the estimated Q-values and the actual Q-value from our experiences in the environment. The TD error, the value we try to minimize, is the $r + \gamma \max_{a’} Q(s’, a’, w_{old}) - Q(s,a, w)$ term we’ve seen in our Q-learning update.<br><br>

      To train the network, we set
      
      $$w \leftarrow w - \frac12 \alpha_t \nabla_w[r + \gamma \max_{a’} Q(s’, a’, w_{old}) - Q(s,a, w)]^2 = w + \alpha_t(r + \gamma \max_{a’}(s’, a’, w_{old}) - Q(s,a; w))\nabla_w Q(s, a; w)$$

      Finally, we introduce experience replay. We put $(s,a,r,s’)$ into a replay buffer, then do mini batch gradient descent steps by sampling sets of memories from the replay buffer. This means we continually reuse our experiences instead of just learning from each experience once. 

      <!-- We can use a neural network to approximate the Q-value function. This started getting popularized in 2013 and onwards. Let's say for example that we want to implement SARSA in our Deep Q-Network, which means we're given tuples of $(s, a, r, s', a')$ that are pulled from the history. Then, we can set up the following loss equation (W represents the weights of the neural network). (Note: Deep Q-Network can be done with the Q-Learning approach, just change the $Q(s'_n, a'_n)$ below accordingly to $max_{a^*} Q(s'_n, a^*)$, the same difference as in our discrete settings).

      $$\sum_n (Q(s_n, a_n) - [r(s_n, a_n) + \gamma Q_W(s'_n, a'_n)])^2$$

      In this equation, the inside, $Q(s_n, a_n) - [r(s_n, a_n) + \gamma Q_W(s'_n, a'_n)]$ represents the temporal difference error.<br><br>

      Our Goal: Learn the $Q_W(s, a)$ that minimizes TD error on the data we have (and we'll still use $\epsilon$-greedy for picking our actions).<br><br>

      The idea here is that we first collect some data, taking actions by following our policy. Over time, we'll have our experience tuples from our history, and given those tuples, we want to train a Q function that minimizes this loss. Specifically, we are optimizing the loss with respect to the W weights in the neural network that we use to approximate the Q function.<br><br>

      Note: we could update after each action, but usually we update from the history after some actions.<br><br>

      In practice, a few things are very tricky:<br><br>

      1. Choose what mini-batches of history to update with<br><br>

      The reason this becomes a challenge is that by doing function approximation, we now have the opportunity to forget. When we were in our discrete state action space, the $Q(s, a)$ was literally a number sitting there in a matrix, it didn't matter if you didn't touch it, you would always remember if a given state action pair is really bad, for example. But when we do an approximation, especially when taking a mini-batched approach, our neural network might focus its energy on other state action pairs and focus on the penalties in the loss of just those. If we stop penalizing a particular state action pair (which depends on what we choose to include and exclude from our mini-batch), learning that pair isn't that important anymore, so over you might forget it. If you happen to forget to include state action pair that has a horrible reward in your mini-batch, then you might forget that that state action pair is bad at all, over time, and later take it by accident. Hence, we see that how you select the mini-batch is quite tricky.<br><br>

      2. Learning with two Q's in the loss isn't super stable.<br><br>

      In the objective above, we have two Q terms. $Q_W(s_n, a_n)$, as well as $Q_W(s'_n, a'_n)$ are both in the loss function. Recall, back in simple situations like least squares regression, the $f(x)$ only appeared once in the loss: $(y - f(x))^2$. It wasn't being evaluated in two different places, it was just evaluated at one place. Beyond the 2013 DQN Paper, and beyond this verys simple version of the loss function, there's been lots of research efforts that have addressed these problems. If you are in the position of needing to try out something like a DQN, definitely invest the effort into more recent versions of this, definitely don't implement what's written down here in lecture. This is mainly here just for the intuition and the big ideas: that we are miniziming TD error for the data that we have in order to help us hopefully learn a good Q function. -->

      <h2 id="recap22_4">Policy Learning</h2>

      Another model-free learning method is policy learning, where we adopt a differentiable policy $\pi_{\theta}(a | s)$ with parameters $\theta$. We’ll learn the policy directly, without any Q-values, models, or planning. This is useful for cases where

      <ul>
        <li>There are continuous actions that can’t be stored in a table, e.g. if we could choose any magnitude of force to apply during an action.</li>
        <li>The policy could be much simpler than the Q-table, making it easier to learn directly. What if we set up a huge Q-table, only to find that the optimal policy is to go right in every state?</li>
        <li>Easier to incorporate expert knowledge. Maybe we want to initialize our model with suggestions from an expert (like a human video game player) about what to do. The human player likely has an idea of what a starting policy should look like, but is less likely to have some internal Q-table.</li>
      </ul>

      To improve our policy, we use gradient ascent to maximize expected reward.

      We set $$\theta \leftarrow \theta + \alpha_t \nabla_{\theta} J(\theta)$$

      Where $J$ is a measure of the agent’s performance. Specifically, $J$ is the expected reward.

      $$J(\theta) = \mathbb E_h [r(h)]$$

      Where $r(h) = \sum_t r_t$, and $h = (s,a,r,s’,a’,…)$ is the history of experiences we are considering. Also, let $\mu$ be the likelihood of a history:

      $$\mu_{\that}(h) = \prod_t \pi_{\theta}(a_t | s_t)p(s_{t + 1} | s_t, a)$$

      We can start to rewrite our update value using what we’ve defined above:

      $$\nabla_{\theta}J(\theta) = \nabla_{\theta} \mathbb E_h[r(h)] = \nabla_{\theta}\int_h \mu_{\theta}(h)r(h)dh = \int_h r(h) \nabla_{\theta}\mu_{\theta}(h)dj$$

      However, now we have a problem. We’re using $\mu_{\theta}(h)$, which this depends on our transition model because it incorporates $p(s_{t + 1} | s_t, a)$. But isn’t the whole point of model-free learning that we don’t need to learn the transition model? To get around this, we use a clever identity that comes from the fact that $\frac{d}{dx} \ln f(x) = \frac{f’(x)}{f(x)}$, so then

      $$\mu_{\theta}\nabla_{\theta}\ln(\mu_{\theta}) = \mu_{\theta}\frac{\nabla_{\theta}\mu_{\theta}}{\mu_{\theta}} =\nabla_{\theta}\mu_{\theta} $$

      By plugging this in, our expression from earlier becomes

      $$\int_h r(h) \mu_{\theta}(h)\nabla_{\theta}[\sum_t \ln \pi_{\theta}(a_t | s_t) + \underbrace{\sum_t \ln p(s_t|s_t, a_t)}{*}]dh$$ 

      Observe that (*) does not depend on \theta, so its gradient with respect to $\theta$ will be 0. Then we can just take it out of the expression, removing the transition probability term we were having trouble with.! Continuing to simplify, we get

      $$\mathbb E_h[r(h) \sum_t \nabla_{\theta} \ln \pi_{\theta}(a_t | s_t)] = \mathbb E_h [\sum_t \nabla_{\theta} \ln \pi_{\theta}(a_t | s_t)r_t]$$

      Finally, this is something we can calculate. We can approximate the expectation by sampling experiences from $h$. Note that this is on-policy because we are following our policy to get the update. 

      The SGD update step ends up being $$\theta \leftarrow \theta + \alpha_t \sum_t \nabla_{\theta} \pi_{\theta}(a_t | s_t)r_t$$


      <!-- This is actually an older approach, and predates DQNs. While it is more computationally expensive, it is a much more stable approach, and actually is the approach Finale uses in her research. It's broken down into some steps:<br><br>

      <h4>Step One: Fit $R_{W_R}(s, a)$ with some function approximator</h4>

      $$\sum_n (r - R_{W_R}(s_n, a_n))^2$$

      We can just fit this loss function above, as this is just standard superivised learning. This is because we have the s, a, and r, so we can learn a function tht goes from s and a to r. We add an R subscript just to emphasize that this is the set of weights for the neural network that is approximating R (as there will be different W's in the later steps).

      <h4>Step Two: Fit $Q_{1, W_1}(s, a)$ using our learned $R_{W_R}(s, a)$</h4>

      Now suppose that we have two time steps to consier, or in other words, two actions to take. To do this, we incorporate R. The R we did above kind of represents $Q_0$ in some sense: as in, if we had only one action to take, in state s, then R is the reward you would you expect to get. Because of this, we can use R to help us define our $Q_1$:

      $$Q_{1, W_1}(s, a) = R_{W_R}(s, a) + \gamma max_{a^*}R_{W_R}(s', a^*)$$

      The $R_{W_R}(s, a)$ represents now, and the $\gamma max_{a^*}R_{W_R}(s', a^*)$ term represents the future. Since $R_{W_R}(s, a)$ is fit already, this whole term on the right side becomes a number. This is great, because now we have another supervised learning problem. Our loss this time will be:<br><br>

      $$\sum_n (Q_{1,target}(s,a) - Q_{1,W_1}(s, a))^2$$

      Where $Q_{1,target}(s,a)$ represents the $R_{W_R}(s, a) + \gamma max_{a^*}R_{W_R}(s', a^*)$ term, the target that we are trying to apprxomiate. Essentially, we are optimizing this loss function to find the best $W_1$ set weights for this $Q_1$ approximator.

      <h4>Step Three: Iterate given $Q_k$, $W_k$</h4>

      Now that we have our previous two steps down, we can see how we can iterate for a general k number of steps. We have the following:

      $$Q_{k+1, W_{k+1}}(s, a) = R_{W_R}(s, a) + \gamma max_{a^*} Q_{k,W_k}(s', a^*)$$

      So this time, our target is $R_{W_R}(s, a) + \gamma max_{a^*} Q_{k,W_k}(s', a^*)$, which is actually fixed, from previous iterations. And our goal this time is to learn $W_{k+1}$, the most up to date weights for our most up to date (our most iterated upon) Q function.<br><br>

      The big difference between Deep Q-Network (DQN) and Fitted Q Iteration (FQI), is that in DQN's we are trying to solve the whole problem at once. Overall, it's saying, here's the final TD error of what you need to learn, just find the ultimate weights that will optimize it (which ends up quite tricky, since the Q function shows up in two places like mentioned above). In the FQI's (what we just covered in this section) however, you're fitting it iteratively, where one side of the equation is always known. We're going one step into the future at a time, and we always know the apprxomiate rewards at the last time step before we fit the next Q. This will computationally take more effort: in a similar fashion to value iteration, let's say you have a lot of iterations you need before you converge, say 50, here what you'll need to do is to complete 50 supervised learning problems (which could take a while). However, it all depends on the situation, in terms of which method is better. Soemtimes, say you had a very limited dataset, then maybe 50 well behaved supervised learning problems is indeed better than 1 Deep Q Learning supervised learning problem, since it's important to have stable and reliable solutions. But other times, you might have a large dataset, and the DQN may be the way to go. -->

      <h2 id="recap22_5">AlphaGo</h2>

      Go has historically been seen as the hardest game for AI to learn how to play. The game is incredibly complex, with a much larger state and action space than chess. For the rest of class, we’ll discuss how AlphaGo managed to beat a Go champion in 2016. The documentary we watched snippets of in class can be found <a href=“https://www.youtube.com/watch?v=WXuK6gekU1Y”>here</a>, and a full paper about AlphaGo <a href=“https://www.nature.com/articles/nature16961”>here</a>. For representing the state of a game, the team decided on hand-crafted per-position features to reduce the enormous state size to a $19 \times 19 \times 48$ space, with $48$ features for each position on the $19 \times 19$ board. For training, a combination of supervised learning and reinforcement learning was used. <br><br>

      A model $\pi_{SL}$ came from supervised learning, where a deep net was trained to predict expert moves using log likelihood loss. This took three weeks of training on 50 GPUs. Then, the team trained a separate model $\pi_FAST$ was a linear softmax model trained with log likelihood loss and the hand coded features. For the reinforcement learning section, a neural network was initialized from the $\pi_SL$ model. The reward function was +1 for a win, -1 for a loss, and 0 for a draw. The model engaged in self-play games with earlier versions of itself over the course of a day, with updates done using the policy gradient method we discussed earlier.  Finally, supervised learning was used to train an estimate of $V_{\theta}$, where the goal was to predict whether the current state would result in a win or a loss. Trained for 1 week on 50 GPUs on 30m positions.<br><br>

      Using these models, how did AlphaGo selected moves using a guided Monte Carlo tree search. This is computationally very expensive, using 48 CPUs, 8 GPUs, and 40 compute threads. Essentially, the agent builds a lookahead tree “looking into” the future, which branches out depending on the actions taken (the actions selected to incorporate in the tree are decided using $\pi_SL$ and Q values backed up from leaves). Once a leaf of the tree is reached, $\pi_fast$ and $V_{\theta}$ are used to approximate the value of the leaf state. <br><br>

      Note: \pi_\theta wasn’t even used. The team found that $\pi_SL$ was more useful.


      <!-- Now, let us make sure we've covered all three approaches. The goal is to provide you a full menu of all the things that exist, so you can look up what's relevant for you.<br><br>

      So far, we've covered value-based approaches. The second category is policy-based approaches. Both value-based and policy-based approaches fall under model-free approaches, because they both aim to learn the value function directly without approximating the model. Then, our third category is model-based approaches, which aims to learn an approximate model based on the agent's experiences, and once it actually learns the T and R functions, applies the methods we learned in how to solve the planning problem. More detail below.

      <h4>Recap of Value-Based Approaches (Model-Free)</h4>

      As a recap, we saw that in the discrete case, we had SARSA and Q-Learning, and in the continuous case, we saw Deep Q-Networks and Fitted Q Iteration. We had focused primarily on value-based approaches only because they're not only effective, but also because they're a bit simpler to understand with regards to the intuition. But depneding on the situation, other methods could be better (or worse).<br><br>

      <h4>Policy Based Approaches (Model-Free)</h4>

      Here, we hope to directly learn $pi_\theta(s, a)$.<br><br>

      These are good ideas if we believe the policy $\pi$ is simpler than the environment. Say you have a pencil on your palm and you're try to balance it. The physics (in other words, the environment) of this involves torques, acceleraiton, angular memoment, etc. maybe that model is complicated but the policy is simple: you want to move your hand in the direction that the pencil is staring to fall. If the policy is simple, maybe we don't bother learning the environment or the Q Function in the first place, you just want to learn the policy you want to take.<br><br>

      Approaches use "policy gradients" (this is the key) to take derivatives of the loss with respect to $\theta$.<br><br>

      Note: There's an entire class of actor-critic algorithms that make this efficient by combining value based and policy based methods together.<br><br>

      <h4>Model-Based Approaches</h4>

      Here, like mentioned before, in model-based approaches, we aim to learn the model first. Formally it's well described in this process:<br><br>

      Iterate between:
      <ol>
        <li>Use data to learn T, R (supervised learning)</li>
        <li>Solve T, R, for $\pi^*$ (planning)</li>
      </ol>

      Basically, it's saying we should use the methods we already know to solve the problem. Let's first figure out the model using supervised learning, then just re-implement our solutions to MDPs.<br><br>

      However, thought it sounds simple, there are some caveats. It's important to carefully figure out how to explore to get accurate T, R where it matters. One approach is sampling T, R which gets some randomness that can be helpful.  Another approach is to assume optimality under uncertainty: as in, if R, T, unknown, presume the best. In this method, if you don't have data for certain areas with respect to the T and R, you have to assume something, so you might assume that it was amazing. If the agent does get there, it will try to do what's unknown because the model says its good, and in real life it either is truly great, and you win, or its horrible, but now you have the data for that area so you can make a better plan for later. There are plenty of approaches to how to determine the T and R. What's difficult is in model-based learning, if all your learning takes place in one region, you might never explore and find out what happpens in other cases.<br><br>

      <h4>Final Note</h4>

      A final reminder: we have assumed that S is given, but in real life, need to identify what the sufficient statistic truly is, as in, what is truly the minimum amount of information that we can use to define the state in which we have all that we need to predict what happens next. There is a lot of literature specifically targetted on how we can create this sufficient statistic given the history, but this is not stuff you'll need to know for the purposes of this course.

      <h2 id="recap22_6">Optional Material</h2>

      A student asked Finale to add more details on the model based and policy based approaches. Below are more notes (this is completely optional):

      Note: Some additional good resources are: Sergey Levin's Deep RL Course Notes, Chapter 8 and 13 in S&B.<br><br>

      <h4>Examples of Model Based</h4>

      <ol>
        <li><a href="https://ie.technion.ac.il/~moshet/brafman02a.pdf">R-Max Algorithms</a></li>
        <li><a href="https://papers.nips.cc/paper/5185-more-efficient-reinforcement-learning-via-posterior-sampling.pdf">Posterior Sample RL</a></li>
      </ol>

      <h4>Policy Gradient Derivation</h4>

      Note: below is just a vanilla example. In real life, you should use Trust Region Policy Optimization (TRPO), Proximal Policy Optimization (PPO), some actor-critic approach.<br><br>

      Let $r(h)$ be $\sum_t r_t$ for history h<br><br>
      Let $J(\theta) = E_{\pi_\theta}[r(h)]$ (this is the value, undiscounted, which is what policy gradient folks usually use)<br><br>

      Goal: get $\nabla_\theta J$ so we can update $\theta$ to become $\theta + \alpha \nabla_\theta J$ for $\pi_\theta(s, a)$

      <div class="text-center">
        <img src="{{ site.baseurl }}/images/recap22_2.png" style="width:70%"  alt="Bonus Material"></img>
      </div> -->
</section>

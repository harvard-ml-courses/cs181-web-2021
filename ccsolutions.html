---
layout: page
title: Concept Check Solutions
mathjax: true
weight: 0
---

<section class="main-container text">
    <div class="main">
      <ul>
        <li>
          Lecture 2 - Linear Regression (January 28, 2021)
          <ul>
            <li>Question A: Graph 1 is Line 2, Graph 2 is Line 1, Graph 3 is Line 2
            <li>Question B: Graph 1: one or more large residuals, Graph 2: an extreme x value, Graph 3: a pattern in the residuals</li>
            <li>Question C: Colinearity between height and leg length</li>
          </ul>
        </li>

        <li>
          Lecture 4 - Linear Classification (February 4, 2021)
          <ul>
            <li>Q1: Hinge and Logistic</li>
            <li>Q2: 0-1</li>
            <li>Q3: Logistic (Hinge is not technically correct, although it is  differentiable almost everywhere)</li>
            <li>Q4: Hinge and Logistic</li>
            <li>Q5: Logistic</li>
          </ul>
        </li>

        <li>
          Lecture 5 - Probabilistic Classification (February 9, 2021)
          <ul>
            <li>QA: 0/1 Lines 2 and 3, Hinge Lines 2 and 3, Logistic line 3</li>
            <li>QB: Line 2</li>
            <li>QC: Yes, the unlabeled data seems to help us here, although might not always help</li>
          </ul>
        </li>

        <li>
          Lecture 6 - Probabilistic Classification (February 9, 2021)
          <ul>
            <li>A: The difference in training and testing may be coming from variance. Adding more data will help for model A. Model B has already been fit, so adding more data may not help (although it may not be the best fit, depending if another model can reach test/validation accuracy of >0.7). </li>
            <li>B: Regularization or ensembles might help because of the high variance in model A. For model B, regularization and early stopping may not be as useful. </li>
            <li>C: If there is high variance in model A, given a new dataset we can expect a different model fit. For model B, we expect similar classifications because it has tended to underfit the data, meaning that most predictions on new datasets will have high bias and low variance. </li>
          </ul>
        </li>

        <li>
          Lecture 10 - Max Margin (February 26, 2021)
          <ul>
            <li>Q1: Removing any of the three points will change the max margin boundary </li>
            <li>Q2: For very large C, the optimal decision boundary will try to separate the data if possible. As C increases, the formulation is more able to "bend" with the data  </li>
            <li>Q3: Lower regularization ("may overfit"!)  </li>
          </ul>
        </li>
      </ul>
    </div>
</section>
